!Experiment

name: bert

pipeline:
  dataset: !TabularDataset.from_path
    train_path: {top_level}/tests/data/dummy_tabular/train.csv
    val_path: {top_level}/tests/data/dummy_tabular/val.csv
    sep: ','
    transform:
      text: !BertTextField
        alias: 'bert-base-uncased'
      label: !LabelField

  # teacher: !TextClassifier
  #   embedder: !BertEmbedder
  #     alias: 'bert-base-uncased'
  #     pool: True
  #   output_layer: !SoftmaxLayer
  #     input_size: !@ teacher[embedder].hidden_size
  #     output_size: !@ dataset.label.vocab_size

  # finetune: !Trainer
  #   dataset: !@ dataset
  #   train_sampler: !BaseSampler
  #     batch_size: 16
  #   val_sampler: !BaseSampler
  #     batch_size: 16
  #   model: !@ teacher
  #   loss_fn: !torch.NLLLoss
  #   metric_fn: !Accuracy
  #   optimizer: !AdamW
  #     params: !@ finetune[model].trainable_params
  #     lr: 0.00005
  #   max_steps: 1
  #   iter_per_step: 1
